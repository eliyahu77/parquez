{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Parquez pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/User/align_mlrun.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the mlrun project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import new_project, mlconf\n",
    "\n",
    "#project_name = '-'.join(filter(None, ['getting-started-iris', getenv('V3IO_USERNAME', None)]))\n",
    "project_name = \"parquez\"\n",
    "project_path = path.abspath('./')\n",
    "project = new_project(project_name, project_path)\n",
    "project.save()\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlconf.artifact_path or path.abspath('./data')\n",
    "# {{run.uid}} will be substituted with the run id, so output will be written to different directoried per run\n",
    "artifact_path = path.join(out, '{{run.uid}}')\n",
    "%env PYTHONPATH=./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name =  'aviaigz/parquez:0.6.0-rc12'\n",
    "\n",
    "project.set_function(\"functions/validate_input.py\", 'validate', kind='job', image=image_name)\n",
    "project.set_function(\"functions/get_table_schema.py\", 'get_schema', kind='job', image=image_name)\n",
    "project.set_function(\"functions/create_parquet_table.py\", 'create_parquet', kind='job', image=image_name)\n",
    "project.set_function(\"functions/create_unified_view.py\", 'create_unified_view', kind='job', image=image_name)\n",
    "project.set_function(\"functions/parquet_add_partition.py\", 'parquet_add_partition', kind='job', image=image_name)\n",
    "project.set_function(\"functions/delete_kv_partition.py\", 'delete_kv_partition', kind='job', image=image_name)\n",
    "project.set_function(\"functions/run_scheduler.py\", 'run_scheduler', kind='job', image=image_name)\n",
    "project.set_function(\"functions/parquetinizer.py\", 'parquetinizer', kind='job', image=image_name)\n",
    "project.set_function(\"functions/run_scheduler.py\", 'run_scheduler', kind='job', image=image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deploy kv-to-parquet function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function\n",
    "#should be converted to an image (spark operator)\n",
    "spark_job = new_function(kind='spark', command='/User/parquez/functions/kv_to_parquet.py', name='kv-to-parquet') # /User isn't supported at this stage\n",
    "project.set_function(spark_job)\n",
    "project.func('kv-to-parquet').with_driver_limits(cpu=\"4000\")\n",
    "project.func('kv-to-parquet').with_driver_requests(cpu=2, mem=\"512m\") # gpu_type & gpus=<number_of_gpus> are supported too\n",
    "project.func('kv-to-parquet').with_executor_limits(cpu=\"4000\")\n",
    "project.func('kv-to-parquet').with_executor_requests(cpu=2, mem=\"512m\")\n",
    "project.func('kv-to-parquet').with_igz_spark() # Adds fuse, daemon & iguazio's jars support\n",
    "project.func('kv-to-parquet').deploy()# Rebuilds the image with MLRun - This is needed in order to support artifact logging etc. This step is too long (~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mount_v3io\n",
    "\n",
    "# create-kv-view\n",
    "project.set_function(\"functions/create_kv_view.py\", 'create-kv-view', kind='job', image=image_name)\n",
    "project.func('create-kv-view').apply(mount_v3io())\n",
    "project.func('create-kv-view').set_env('PYTHONPATH', project_path)\n",
    "project.func('create-kv-view').deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet-add-partition\n",
    "project.set_function(\"functions/parquet_add_partition.py\", 'parquet-add-partition', kind='job', image=image_name)\n",
    "project.func('parquet-add-partition').apply(mount_v3io())\n",
    "project.func('parquet-add-partition').set_env('PYTHONPATH', project_path)\n",
    "project.func('parquet-add-partition').deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete-kv-partition\n",
    "project.set_function(\"functions/delete_kv_partition.py\", 'delete-kv-partition', kind='job', image=image_name)\n",
    "project.func('delete-kv-partition').apply(mount_v3io())\n",
    "project.func('delete-kv-partition').set_env('PYTHONPATH', project_path)\n",
    "project.func('delete-kv-partition').deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquetinizer\n",
    "project.set_function(\"functions/parquetinizer.py\", 'parquetinizer', kind='job', image=image_name)\n",
    "project.func('parquetinizer').apply(mount_v3io())\n",
    "project.func('parquetinizer').set_env('PYTHONPATH', project_path)\n",
    "project.func('parquetinizer').deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-step-create-n-run-ml-pipeline\"></a>\n",
    "## Create and Run a Fully Automated ML Pipeline\n",
    "\n",
    "You're now ready to create a full ML pipeline.\n",
    "This is done by using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/), which is integrated into the Iguazio Data Science Platform.\n",
    "Kubeflow Pipelines is an open-source framework for building and deploying portable, scalable machine-learning workflows based on Docker containers.\n",
    "MLRun leverages this framework to take your existing code and deploy it as steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {path.join(project_path, 'workflow.py')}\n",
    "\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "V3IO_ACCESS_KEY = os.environ['V3IO_ACCESS_KEY']\n",
    "V3IO_USERNAME = os.getenv('V3IO_USERNAME')\n",
    "\n",
    "funcs = {}\n",
    "project_path = path.abspath('./')\n",
    "parquez_params = {'view_name':'view_name'\n",
    "         ,'partition_by':'h'\n",
    "         ,'partition_interval':'1h'\n",
    "         ,'real_time_window':'3h'\n",
    "         ,'historical_retention':'24h'\n",
    "         ,'real_time_table_name':'faker'\n",
    "         ,'config_path':'/User/parquez/config/parquez.ini'\n",
    "         ,'user_name':V3IO_USERNAME\n",
    "         ,'access_key':V3IO_ACCESS_KEY          \n",
    "         ,'project_path': project_path\n",
    "         }\n",
    "\n",
    "\n",
    "# Configure function resources and local settings\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    project_path = path.abspath('./')\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())\n",
    "        f.set_env('PYTHONPATH', project_path)\n",
    "        f.spec.artifact_path = 'User/artifacts'\n",
    "        f.spec.service_account='mlrun-api'\n",
    "        \n",
    "        \n",
    "# Create a Kubeflow Pipelines pipeline\n",
    "@dsl.pipeline(\n",
    "    name = \"parquez-pipeline\",\n",
    "    description = \"parquez description\"\n",
    ")\n",
    "def kfpipeline():\n",
    "    \n",
    "#     # clean the tables\n",
    "#     clean = funcs['clean'].as_step(\n",
    "#         name=\"clean\",\n",
    "#         params=parquez_params,\n",
    "#         outputs=['clean']\n",
    "#     )\n",
    "    \n",
    "    # Ingest the data set\n",
    "    validate = funcs['validate'].as_step(\n",
    "        name=\"validate\",\n",
    "        params=parquez_params,\n",
    "#         inputs={'table': clean.outputs},\n",
    "        outputs=['validate']\n",
    "    )\n",
    "    \n",
    "    # Analyze the dataset\n",
    "    schema = funcs['get_schema'].as_step(\n",
    "        name=\"get_schema\",\n",
    "        params = parquez_params,\n",
    "        inputs={'table': validate.outputs},                       \n",
    "        outputs=['schema']\n",
    "    )\n",
    "    \n",
    "    parquet = funcs[\"create_parquet\"].as_step(\n",
    "        name=\"create_parquet\",\n",
    "        params=parquez_params,\n",
    "        inputs={\"table\": schema.outputs['schema']},\n",
    "        outputs=['create_parquet']\n",
    "    )\n",
    "    \n",
    "    kv_view = funcs[\"create-kv-view\"].as_step(\n",
    "        name=\"create_kv_view\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': parquet.outputs},\n",
    "        outputs=['kv_view']\n",
    "    )\n",
    "    \n",
    "    unified_view = funcs[\"create_unified_view\"].as_step(\n",
    "        name=\"create_unified_view\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': kv_view.outputs},\n",
    "        outputs=['unified_view']\n",
    "    )\n",
    "    \n",
    "    unified_view = funcs[\"run_scheduler\"].as_step(\n",
    "        name=\"run_scheduler\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': unified_view.outputs},\n",
    "        outputs=['run_scheduler']\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-register-workflow\"></a>\n",
    "#### Register the Workflow\n",
    "\n",
    "Use the `set_workflow` MLRun project method to register your workflow with MLRun.\n",
    "The following code sets the `name` parameter to the selected workflow name (\"main\") and the `code` parameter to the name of the workflow file that is found in your project directory (**workflow.py**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the workflow file as \"main\"\n",
    "project.set_workflow('main', 'workflow.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = project.run(\n",
    "    'main',\n",
    "    arguments={}, \n",
    "    \n",
    "    artifact_path=path.abspath(path.join('pipeline','{{workflow.uid}}'),\n",
    "    \n",
    "                              )\n",
    "    ,dirty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import get_run_db\n",
    "get_run_db().list_schedules('parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!printenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
