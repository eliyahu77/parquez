{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests colorlog PyHive mlrun kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/parquez\n",
      "Project name: parquez\n"
     ]
    }
   ],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import new_project, mlconf\n",
    "\n",
    "#project_name = '-'.join(filter(None, ['getting-started-iris', getenv('V3IO_USERNAME', None)]))\n",
    "project_name = \"parquez\"\n",
    "project_path = path.abspath('./')\n",
    "project = new_project(project_name, project_path)\n",
    "project.save()\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlconf.artifact_path or path.abspath('./data')\n",
    "# {{run.uid}} will be substituted with the run id, so output will be written to different directoried per run\n",
    "artifact_path = path.join(out, '{{run.uid}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONPATH=./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project.set_function(\"parquez.py\", 'parquezrun', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7ff461053c50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(\"validate-input.py\", 'validate', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local, mount_v3io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.func('validate').apply(mount_v3io())\n",
    "project.func('validate').set_env('PYTHONPATH', project_path)\n",
    "project.func('validate').spec.artifact_path = 'User/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7ff4304b9390>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(\"get_table_schema.py\", 'get_schema', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.func('get_schema').apply(mount_v3io())\n",
    "project.func('get_schema').set_env('PYTHONPATH', project_path)\n",
    "project.func('get_schema').spec.artifact_path = 'User/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7ff4304bd410>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(\"create_parquet_table.py\", 'create_parquet', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.func('create_parquet').apply(mount_v3io())\n",
    "project.func('create_parquet').set_env('PYTHONPATH', project_path)\n",
    "project.func('create_parquet').spec.artifact_path = 'User/artifacts'\n",
    "project.func('create_parquet').spec.service_account='mlrun-api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7ff43064f850>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(\"create_kv_view.py\", 'create_kv_view', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.func('create_kv_view').apply(mount_v3io())\n",
    "project.func('create_kv_view').set_env('PYTHONPATH', project_path)\n",
    "project.func('create_kv_view').spec.artifact_path = 'User/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7ff4307cf090>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(\"create_unified_view.py\", 'create_unified_view', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.func('create_unified_view').apply(mount_v3io())\n",
    "project.func('create_unified_view').set_env('PYTHONPATH', project_path)\n",
    "project.func('create_unified_view').spec.artifact_path = 'User/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7ff43065a290>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_function(\"run_parquez_interval.py\", 'run_parquez_interval', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.func('run_parquez_interval').apply(mount_v3io())\n",
    "project.func('run_parquez_interval').set_env('PYTHONPATH', project_path)\n",
    "project.func('run_parquez_interval').spec.artifact_path = 'User/artifacts'\n",
    "project.func('run_parquez_interval').spec.service_account='mlrun-api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-07-22 08:40:57,509 starting run run-parquez-interval uid=efe6a5b6dc8e48b599e90effd9818a88  -> http://mlrun-api:8080\n",
      "[mlrun] 2020-07-22 08:40:57,764 Job is running in the background, pod: run-parquez-interval-lq6k7\n",
      "[mlrun] 2020-07-22 08:41:03,068 warning!, server (0.4.10) and client (0.4.9) ver dont match\n",
      "[mlrun] 2020-07-22 08:41:03,109 starting local run: main.py # main\n",
      "[mlrun] 2020-07-22 08:41:05,236 logging run results to: http://mlrun-api:8080\n",
      "[mlrun] 2020-07-22 08:41:05,246 warning!, server (0.4.10) and client (0.4.9) ver dont match\n",
      "[mlrun] 2020-07-22 08:41:05,292 loading configuration\n",
      "[mlrun] 2020-07-22 08:41:05,292 ['bin', 'boot', 'dev', 'etc', 'home', 'lib', 'lib64', 'media', 'mnt', 'opt', 'proc', 'root', 'run', 'sbin', 'srv', 'sys', 'tmp', 'usr', 'var', 'User', 'main.py', '.dockerenv']\n",
      "[mlrun] 2020-07-22 08:41:05,295 generating cronJob\n",
      "[mlrun] 2020-07-22 08:41:05,296 using in-cluster config.\n",
      "[mlrun] 2020-07-22 08:41:05,298 parquez/sh/parquetinizer.sh faker '1 days' '7 days' 'h' 'parquez' 'default' 'Parquet' '6'\n",
      "[mlrun] 2020-07-22 08:41:05,298 using in-cluster config.\n",
      "[mlrun] 2020-07-22 08:41:05,492 10.200.0.48\tdefault-tenant\tshell-77988bb5c9-5lc25\n",
      "[mlrun] 2020-07-22 08:41:15,165 Response: parquez_dir:parquez\n",
      "kv_table_name:faker\n",
      "kv_window:1 days\n",
      "historical_window:7 days\n",
      "partition_by:h\n",
      "v3io_container:parquez\n",
      "hive_schema:default\n",
      "compression_type:Parquet\n",
      "coalesce:\n",
      "user is: iguazio\n",
      "current UTC time: Wed Jul 22 08:41:05 UTC 2020\n",
      "1 days ago: Tue Jul 21 08:41:05 UTC 2020\n",
      "year is: 2020\n",
      "month is: 07\n",
      "day is: 21\n",
      "hour is: 08\n",
      "current year is: 2020\n",
      "current month is: 07\n",
      "current day is: 22\n",
      "current hour is: 08\n",
      "hitorical window : Wed Jul 15 08:41:05 UTC 2020\n",
      "year is: 2020\n",
      "month is: 07\n",
      "day is: 15\n",
      "hour is: 08\n",
      "source is: v3io://parquez/faker/year=2020/month=07/day=21/hour=08\n",
      "target is: v3io://parquez/faker_Parquet/year=2020/month=07/day=21/hour=08\n",
      "parquetToDelete is: v3io://parquez/faker_Parquet/year=2020/month=07/day=15/hour=08\n",
      "iguazio   39392  39342  0 08:41 ?        00:00:00 grep parquez-assembly\n",
      "No parquez-assembly process is running. Continue...\n",
      "query: where year >= 2020 AND month >= 07 AND ( day >= 21 AND day <= 22 ) AND hour > 08\n",
      "20/07/22 08:41:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/07/22 08:41:07 WARN deploy.DependencyUtils: Local jar /igz/java/libs/v3io-spark2-tools_2.11-435956327165934200101.jar does not exist, skipping.\n",
      "20/07/22 08:41:07 WARN deploy.SparkSubmit$$anon$2: Failed to load io.iguaz.v3io.spark2.tools.KVToParquet.\n",
      "java.lang.ClassNotFoundException: io.iguaz.v3io.spark2.tools.KVToParquet\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:806)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "20/07/22 08:41:07 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "20/07/22 08:41:07 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-708f392f-feb6-48d5-a8a6-25a507e5687f\n",
      "parquez/sh/parquetinizer.sh: line 230: spark_command: command not found\n",
      "KV to parquet finished with success\n",
      "parquez/sh/parquetinizer.sh: line 239: popd: directory stack empty\n",
      "parquez/sh/alter_kv_view.sh faker '1 days'\n",
      "parquez/sh/alter_kv_view.sh: line 11: venv/bin/activate: No such file or directory\n",
      "alter_kv_view faker 1 days\n",
      "python: can't open file 'core/alter_kv_view.py': [Errno 2] No such file or directory\n",
      "alter table command :add\n",
      "hive schema:default\n",
      "parquet table name:faker_Parquet\n",
      "year:2020\n",
      "month:07\n",
      "day:21\n",
      "hour:08\n",
      "target: v3io://parquez/faker_Parquet/year=2020/month=07/day=21/hour=08\n",
      "partition by:h\n",
      "HIVE PATH:kubectl -n default-tenant exec $(kubectl -n default-tenant get pods --no-headers -o custom-columns=\":metadata.name\" | grep shell) -- /bin/bash -c \"/hive/bin/hive -hiveconf hive.metastore.uris=thrift://hive:9083\n",
      "Partition to delete year=2020, month=07, day=21, hour=08\n",
      "execution command:  kubectl -n default-tenant exec $(kubectl -n default-tenant get pods --no-headers -o custom-columns=\":metadata.name\" | grep shell) -- /bin/bash -c \"/hive/bin/hive -hiveconf hive.metastore.uris=thrift://hive:9083 -e 'alter table default.faker_Parquet add partition (year=2020, month=07, day=21, hour=08) location \\\"v3io://parquez/faker_Parquet/year=2020/month=07/day=21/hour=08\\\";'\"\n",
      "Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default-tenant:default\" cannot list resource \"pods\" in API group \"\" in the namespace \"default-tenant\"\n",
      "error: expected 'exec POD_NAME COMMAND [ARG1] [ARG2] ... [ARGN]'.\n",
      "POD_NAME and COMMAND are required arguments for the exec command\n",
      "See 'kubectl exec -h' for help and examples.\n",
      "20/07/22 08:41:11 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "rm: `v3io://parquez/faker/year=2020/month=07/day=21/hour=08': No such file or directory\n",
      "20/07/22 08:41:13 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "rm: `v3io://parquez/faker_Parquet/year=2020/month=07/day=15/hour=08': No such file or directory\n",
      "alter table command :drop\n",
      "hive schema:default\n",
      "parquet table name:faker_Parquet\n",
      "year:2020\n",
      "month:07\n",
      "day:15\n",
      "hour:08\n",
      "target: v3io://parquez/faker_Parquet/year=2020/month=07/day=21/hour=08\n",
      "partition by:h\n",
      "HIVE PATH:kubectl -n default-tenant exec $(kubectl -n default-tenant get pods --no-headers -o custom-columns=\":metadata.name\" | grep shell) -- /bin/bash -c \"/hive/bin/hive -hiveconf hive.metastore.uris=thrift://hive:9083\n",
      "Partition to delete year=2020, month=07, day=15, hour=08\n",
      "execution command:  kubectl -n default-tenant exec $(kubectl -n default-tenant get pods --no-headers -o custom-columns=\":metadata.name\" | grep shell) -- /bin/bash -c \"/hive/bin/hive -hiveconf hive.metastore.uris=thrift://hive:9083 -e 'alter table default.faker_Parquet drop partition (year=2020, month=07, day=15, hour=08) ;'\"\n",
      "Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default-tenant:default\" cannot list resource \"pods\" in API group \"\" in the namespace \"default-tenant\"\n",
      "error: expected 'exec POD_NAME COMMAND [ARG1] [ARG2] ... [ARGN]'.\n",
      "POD_NAME and COMMAND are required arguments for the exec command\n",
      "See 'kubectl exec -h' for help and examples.\n",
      "parquez/sh/parquetinizer.sh: line 259: popd: directory stack empty\n",
      "\n",
      "\n",
      "[mlrun] 2020-07-22 08:41:15,557 run executed, status=completed\n",
      "final state: succeeded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>parquez</td>\n",
       "      <td><div title=\"efe6a5b6dc8e48b599e90effd9818a88\"><a href=\"https://mlrun-ui.default-tenant.app.app-lab-azure-b5157.iguazio-cd1.com/projects/parquez/jobs/efe6a5b6dc8e48b599e90effd9818a88/info\" target=\"_blank\" >...d9818a88</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jul 22 08:41:05</td>\n",
       "      <td>completed</td>\n",
       "      <td>run-parquez-interval</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">host=run-parquez-interval-lq6k7</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">view_name=view_name</div><div class=\"dictlist\">partition_by=h</div><div class=\"dictlist\">partition_interval=1h</div><div class=\"dictlist\">real_time_window=1d</div><div class=\"dictlist\">historical_retention=7d</div><div class=\"dictlist\">real_time_table_name=faker</div><div class=\"dictlist\">config_path=/User/parquez/config/parquez.ini</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result27932d2c-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result27932d2c-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result27932d2c\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result27932d2c-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run efe6a5b6dc8e48b599e90effd9818a88 --project parquez , !mlrun logs efe6a5b6dc8e48b599e90effd9818a88 --project parquez\n",
      "[mlrun] 2020-07-22 08:41:23,147 run executed, status=completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7ff4321e0890>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_path = '/User/artifacts'\n",
    "# #project.func('parquezrun').run()\n",
    "project.func('run_parquez_interval').run( params = {'view_name':'view_name'\n",
    "         ,'partition_by':'h'\n",
    "         ,'partition_interval':'1h'\n",
    "         ,'real_time_window':'1d'\n",
    "         ,'historical_retention':'7d'\n",
    "         ,'real_time_table_name':'faker'\n",
    "         ,'config_path':'/User/parquez/config/parquez.ini'},artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-step-create-n-run-ml-pipeline\"></a>\n",
    "## Create and Run a Fully Automated ML Pipeline\n",
    "\n",
    "You're now ready to create a full ML pipeline.\n",
    "This is done by using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/), which is integrated into the Iguazio Data Science Platform.\n",
    "Kubeflow Pipelines is an open-source framework for building and deploying portable, scalable machine-learning workflows based on Docker containers.\n",
    "MLRun leverages this framework to take your existing code and deploy it as steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {path.join(project_path, 'workflow.py')}\n",
    "\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "\n",
    "funcs = {}\n",
    "parquez_params = {'view_name':'view_name'\n",
    "         ,'partition_by':'h'\n",
    "         ,'partition_interval':'1h'\n",
    "         ,'real_time_window':'1d'\n",
    "         ,'historical_retention':'7d'\n",
    "         ,'real_time_table_name':'faker'\n",
    "         ,'config_path':'/User/parquez/config/parquez.ini'}\n",
    "\n",
    "\n",
    "# Configure function resources and local settings\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())\n",
    "\n",
    "    #functions['serving'].metadata.name = 'getting-started-serving'\n",
    "\n",
    "\n",
    "# Create a Kubeflow Pipelines pipeline\n",
    "@dsl.pipeline(\n",
    "    name = \"parquez-pipeline\",\n",
    "    description = \"parquez description\"\n",
    ")\n",
    "def kfpipeline():\n",
    "    # Ingest the data set\n",
    "    validate = funcs['validate'].as_step(\n",
    "        name=\"validate\",\n",
    "        params=parquez_params,\n",
    "        outputs=['validate']\n",
    "    )\n",
    "    \n",
    "    # Analyze the dataset\n",
    "    schema = funcs['get_schema'].as_step(\n",
    "        name=\"get_schema\",\n",
    "        params = parquez_params,\n",
    "        inputs={'table': validate.outputs},                       \n",
    "        outputs=['schema']\n",
    "    )\n",
    "    \n",
    "    parquet = funcs[\"create_parquet\"].as_step(\n",
    "        name=\"create_parquet\",\n",
    "        params=parquez_params,\n",
    "        inputs={\"table\": schema.outputs['schema']},\n",
    "        outputs=['create_parquet']\n",
    "    )\n",
    "    \n",
    "    kv_view = funcs[\"create_kv_view\"].as_step(\n",
    "        name=\"create_kv_view\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': parquet.outputs},\n",
    "        outputs=['kv_view']\n",
    "    )\n",
    "    \n",
    "    unified_view = funcs[\"create_unified_view\"].as_step(\n",
    "        name=\"create_unified_view\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': kv_view.outputs},\n",
    "        outputs=['unified_view']\n",
    "    )\n",
    "    \n",
    "    unified_view = funcs[\"run_parquez_interval\"].as_step(\n",
    "        name=\"run_parquez_interval\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': unified_view.outputs},\n",
    "        outputs=['run_parquez_interval']\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-register-workflow\"></a>\n",
    "#### Register the Workflow\n",
    "\n",
    "Use the `set_workflow` MLRun project method to register your workflow with MLRun.\n",
    "The following code sets the `name` parameter to the selected workflow name (\"main\") and the `code` parameter to the name of the workflow file that is found in your project directory (**workflow.py**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the workflow file as \"main\"\n",
    "project.set_workflow('main', 'workflow.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = project.run(\n",
    "    'main',\n",
    "    arguments={}, \n",
    "    \n",
    "    artifact_path=path.abspath(path.join('pipeline','{{workflow.uid}}'),\n",
    "    \n",
    "                              )\n",
    "    ,dirty=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
