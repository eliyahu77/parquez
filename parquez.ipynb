{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Parquez pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: mlrun\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install mlrun==0.6.0rc7\n",
    "!pip show mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the mlrun project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/parquez\n",
      "Project name: parquez\n"
     ]
    }
   ],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import new_project, mlconf\n",
    "\n",
    "#project_name = '-'.join(filter(None, ['getting-started-iris', getenv('V3IO_USERNAME', None)]))\n",
    "project_name = \"parquez\"\n",
    "project_path = path.abspath('./')\n",
    "project = new_project(project_name, project_path)\n",
    "project.save()\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./\n"
     ]
    }
   ],
   "source": [
    "out = mlconf.artifact_path or path.abspath('./data')\n",
    "# {{run.uid}} will be substituted with the run id, so output will be written to different directoried per run\n",
    "artifact_path = path.join(out, '{{run.uid}}')\n",
    "%env PYTHONPATH=./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-28 14:10:30,752 [info] running build to add mlrun package, set with_mlrun=False to skip if its already in the image\n",
      "> 2020-12-28 14:10:30,764 [info] starting remote build, image: .mlrun/func-parquez-run_parquez_interval-latest\n",
      "E1228 14:11:23.552542       1 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.\n",
      "\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors\n",
      "\u001b[36mINFO\u001b[0m[0040] Retrieving image manifest datanode-registry.iguazio-platform.app.dev39.lab.iguazeng.com:80/iguazio/spark-app:3.0_b5960_20201227102651 \n",
      "\u001b[36mINFO\u001b[0m[0040] Retrieving image manifest datanode-registry.iguazio-platform.app.dev39.lab.iguazeng.com:80/iguazio/spark-app:3.0_b5960_20201227102651 \n",
      "\u001b[36mINFO\u001b[0m[0040] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0040] Retrieving image manifest datanode-registry.iguazio-platform.app.dev39.lab.iguazeng.com:80/iguazio/spark-app:3.0_b5960_20201227102651 \n",
      "\u001b[36mINFO\u001b[0m[0040] Retrieving image manifest datanode-registry.iguazio-platform.app.dev39.lab.iguazeng.com:80/iguazio/spark-app:3.0_b5960_20201227102651 \n",
      "\u001b[36mINFO\u001b[0m[0040] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0040] Unpacking rootfs as cmd RUN pip install mlrun==0.6.0-rc7 requires it. \n"
     ]
    }
   ],
   "source": [
    "from mlrun.run import new_function\n",
    "from mlrun import mount_v3io\n",
    "#project.set_function(\"functions/clean_parquez.py\", 'clean', kind='job', image='aviaigz/parquez')\n",
    "project.set_function(\"functions/validate_input.py\", 'validate', kind='job', image='aviaigz/parquez')\n",
    "project.set_function(\"functions/get_table_schema.py\", 'get_schema', kind='job', image='aviaigz/parquez')\n",
    "project.set_function(\"functions/create_parquet_table.py\", 'create_parquet', kind='job', image='aviaigz/parquez')\n",
    "project.set_function(\"functions/create_kv_view.py\", 'create_kv_view', kind='job', image='aviaigz/parquez')\n",
    "project.set_function(\"functions/create_unified_view.py\", 'create_unified_view', kind='job', image='aviaigz/parquez')\n",
    "spark_job = new_function(kind='spark', command='/User/parquez/functions/kv_to_parquet.py', name='run_parquez_interval') # /User isn't supported at this stage\n",
    "\n",
    "project.set_function(spark_job)\n",
    "project.func('run_parquez_interval').with_driver_limits(cpu=\"1300m\")\n",
    "project.func('run_parquez_interval').with_driver_requests(cpu=1, mem=\"512m\") # gpu_type & gpus=<number_of_gpus> are supported too\n",
    "project.func('run_parquez_interval').with_executor_limits(cpu=\"1400m\")\n",
    "project.func('run_parquez_interval').with_executor_requests(cpu=2, mem=\"512m\") # gpu_type & gpus=<number_of_gpus> are supported too\n",
    "project.func('run_parquez_interval').with_igz_spark() # Adds fuse, daemon & iguazio's jars support\n",
    "project.func('run_parquez_interval').spec.replicas = 2 # Number of executors\n",
    "project.func('run_parquez_interval').deploy() # Rebuilds the image with MLRun - This is needed in order to support artifact logging etc. This step is too long (~3 minutes)\n",
    "project.set_function(\"functions/run_scheduler.py\", 'run_scheduler', kind='job', image='aviaigz/parquez')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-step-create-n-run-ml-pipeline\"></a>\n",
    "## Create and Run a Fully Automated ML Pipeline\n",
    "\n",
    "You're now ready to create a full ML pipeline.\n",
    "This is done by using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/), which is integrated into the Iguazio Data Science Platform.\n",
    "Kubeflow Pipelines is an open-source framework for building and deploying portable, scalable machine-learning workflows based on Docker containers.\n",
    "MLRun leverages this framework to take your existing code and deploy it as steps in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {path.join(project_path, 'workflow.py')}\n",
    "\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "V3IO_ACCESS_KEY = os.environ['V3IO_ACCESS_KEY']\n",
    "V3IO_USERNAME = os.getenv('V3IO_USERNAME')\n",
    "\n",
    "funcs = {}\n",
    "project_path = path.abspath('./')\n",
    "parquez_params = {'view_name':'view_name'\n",
    "         ,'partition_by':'h'\n",
    "         ,'partition_interval':'1h'\n",
    "         ,'real_time_window':'1d'\n",
    "         ,'historical_retention':'7d'\n",
    "         ,'real_time_table_name':'faker'\n",
    "         ,'config_path':'/User/parquez/config/parquez.ini'\n",
    "         ,'user_name':V3IO_USERNAME\n",
    "         ,'access_key':V3IO_ACCESS_KEY          \n",
    "         ,'project_path': project_path\n",
    "         ,'shell_pod_name' : 'parquez-shell'       }\n",
    "\n",
    "\n",
    "# Configure function resources and local settings\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    project_path = path.abspath('./')\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())\n",
    "        f.set_env('PYTHONPATH', project_path)\n",
    "        f.spec.artifact_path = 'User/artifacts'\n",
    "        f.spec.service_account='mlrun-api'\n",
    "        \n",
    "        \n",
    "# Create a Kubeflow Pipelines pipeline\n",
    "@dsl.pipeline(\n",
    "    name = \"parquez-pipeline\",\n",
    "    description = \"parquez description\"\n",
    ")\n",
    "def kfpipeline():\n",
    "    \n",
    "#     # clean the tables\n",
    "#     clean = funcs['clean'].as_step(\n",
    "#         name=\"clean\",\n",
    "#         params=parquez_params,\n",
    "#         outputs=['clean']\n",
    "#     )\n",
    "    \n",
    "    # Ingest the data set\n",
    "    validate = funcs['validate'].as_step(\n",
    "        name=\"validate\",\n",
    "        params=parquez_params,\n",
    "#         inputs={'table': clean.outputs},\n",
    "        outputs=['validate']\n",
    "    )\n",
    "    \n",
    "    # Analyze the dataset\n",
    "    schema = funcs['get_schema'].as_step(\n",
    "        name=\"get_schema\",\n",
    "        params = parquez_params,\n",
    "        inputs={'table': validate.outputs},                       \n",
    "        outputs=['schema']\n",
    "    )\n",
    "    \n",
    "    parquet = funcs[\"create_parquet\"].as_step(\n",
    "        name=\"create_parquet\",\n",
    "        params=parquez_params,\n",
    "        inputs={\"table\": schema.outputs['schema']},\n",
    "        outputs=['create_parquet']\n",
    "    )\n",
    "    \n",
    "    kv_view = funcs[\"create_kv_view\"].as_step(\n",
    "        name=\"create_kv_view\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': parquet.outputs},\n",
    "        outputs=['kv_view']\n",
    "    )\n",
    "    \n",
    "    unified_view = funcs[\"create_unified_view\"].as_step(\n",
    "        name=\"create_unified_view\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': kv_view.outputs},\n",
    "        outputs=['unified_view']\n",
    "    )\n",
    "    \n",
    "    unified_view = funcs[\"run_scheduler\"].as_step(\n",
    "        name=\"run_scheduler\",\n",
    "        params=parquez_params,\n",
    "        inputs={'table': unified_view.outputs},\n",
    "        outputs=['run_scheduler']\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-register-workflow\"></a>\n",
    "#### Register the Workflow\n",
    "\n",
    "Use the `set_workflow` MLRun project method to register your workflow with MLRun.\n",
    "The following code sets the `name` parameter to the selected workflow name (\"main\") and the `code` parameter to the name of the workflow file that is found in your project directory (**workflow.py**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the workflow file as \"main\"\n",
    "project.set_workflow('main', 'workflow.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = project.run(\n",
    "    'main',\n",
    "    arguments={}, \n",
    "    \n",
    "    artifact_path=path.abspath(path.join('pipeline','{{workflow.uid}}'),\n",
    "    \n",
    "                              )\n",
    "    ,dirty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import get_run_db\n",
    "get_run_db().list_schedules('parquez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
