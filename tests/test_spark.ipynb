{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlrun==0.6.0-rc7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.run import new_function\n",
    " \n",
    "sj = new_function(kind='spark', command='/User/parquez/functions/kv_to_parquet.py', name='test-spark') # /User isn't supported at this stage\n",
    " \n",
    "sj.with_driver_limits(cpu=\"1300m\")\n",
    "sj.with_driver_requests(cpu=1, mem=\"512m\") # gpu_type & gpus=<number_of_gpus> are supported too\n",
    "sj.with_executor_limits(cpu=\"1400m\")\n",
    "sj.with_executor_requests(cpu=1, mem=\"512m\") # gpu_type & gpus=<number_of_gpus> are supported too\n",
    " \n",
    "sj.with_igz_spark() # Adds fuse, daemon & iguazio's jars support\n",
    "\n",
    "# Args are also supported:\n",
    "# sj.spec.args = ['-arg1', '-arg2']\n",
    " \n",
    "sj.spec.replicas = 2 # Number of executors\n",
    " \n",
    "sj.deploy() # Rebuilds the image with MLRun - This is needed in order to support artifact logging etc. This step is too long (~3 minutes)\n",
    " \n",
    "sr = sj.run(artifact_path=\"/User/artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "\n",
    "def generate_kv_parquet_path(container='parquez',\n",
    "                             table='faker',\n",
    "                             compress_type='parquet',\n",
    "                             partition_by='h',\n",
    "                             real_time_window='0h'):\n",
    "    real_time_window_delta = int(real_time_window[:-1])\n",
    "    print(real_time_window_delta)\n",
    "    from datetime import datetime, timezone, timedelta\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    current_date_path = None\n",
    "    if partition_by == 'h':\n",
    "        current_date_path = (datetime.now(timezone.utc) - timedelta(hours=real_time_window_delta)).strftime(\n",
    "            \"year=%Y/month=%m/day=%d/hour=%H\")\n",
    "    elif partition_by == 'd':\n",
    "        current_date_path = (datetime.now(timezone.utc) - timedelta(days=real_time_window_delta)).strftime(\n",
    "            \"year=%Y/month=%m/day=%d\")\n",
    "    elif partition_by == 'm':\n",
    "        current_date_path = (datetime.now(timezone.utc) - relativedelta(months=real_time_window_delta)).strftime(\n",
    "            \"year=%Y/month=%m\")\n",
    "    elif partition_by == 'y':\n",
    "        current_date_path = (datetime.now(timezone.utc) - relativedelta(years=real_time_window_delta)).strftime(\n",
    "            \"year=%Y\")\n",
    "    kv_path = \"v3io://{}/{}/{}/\".format(container, table, current_date_path)\n",
    "    parquet_path = \"v3io://{}/{}_{}/{}/\".format(container, table, compress_type, current_date_path)\n",
    "    fuse_kv_path =  \"/v3io/{}/{}/{}/\".format(container, table, current_date_path)\n",
    "    print(\"kv path: {} , parquet_path : {} :fuse_kv_path {}\".format(kv_path, parquet_path, fuse_kv_path))\n",
    "    return {'kv_path': kv_path, 'parquet_path': parquet_path, 'fuse_kv_path': fuse_kv_path}\n",
    "\n",
    "paths = generate_kv_parquet_path()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.isdir(paths['fuse_kv_path']):  \n",
    "    print('wow')\n",
    "else:\n",
    "    print(\"Directory {} Doesnt exist\".format(paths['fuse_kv_path']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.isdir('/v3io/parquez/faker/year=2020/'):  \n",
    "    print('wow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# METHOD I:\n",
    "#   Update Spark configurations of the following two extraClassPath with the JDBC driver location\n",
    "#   prior to initiating a Spark session:\n",
    "#      spark.driver.extraClassPath\n",
    "#      spark.executor.extraClassPath\n",
    "#\n",
    "# NOTE:\n",
    "# If you don't connnect to mysql, replace the mysql's connector by the other database's JDBC connector \n",
    "# in the following two extraClassPath.\n",
    "#\n",
    "# Initiate a Spark Session\n",
    "spark = SparkSession.builder.appName(\"Spark JDBC to Databases - ipynb\").getOrCreate()\n",
    "\n",
    "df = spark.read.format('io.iguaz.v3io.spark.sql.kv').load('v3io://parquez/faker/year=2020/month=12/day=22/hour=11')\n",
    "df.show()\n",
    "#df.repartition(coalesce).write.mode('overwrite').parquet(paths['parquet_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
